{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.cmu.edu:3128\"\n",
    "os.environ[\"https_proxy\"] = \"https://proxy.cmu.edu:3128\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2b1b2ee7dafa>:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from adamp import AdamP\n",
    "from torchsummaryX import summary\n",
    "from utlis import calculate_threshold\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data/wav_resampled16kHz\"\n",
    "# root = \"./data/musicnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save MFCC files for train, val, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load train and validation data\n",
    "\n",
    "from calendar import day_abbr\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data_path, context, offset=0, partition= \"train\", limit=-1, shuffle=True, Normalization=True, mfcc_list=[\"mfcc\", \"mfcc_320\", \"mfcc_480\"]): # Feel free to add more arguments\n",
    "\n",
    "        self.context = context\n",
    "        self.offset = context\n",
    "        self.data_path = data_path\n",
    "        self.normalization = Normalization\n",
    "        self.mfcc_list = mfcc_list\n",
    "        \n",
    "        self.mfcc_list = []\n",
    "        self.transcripts = []\n",
    "        for index, mfcc_file in enumerate(mfcc_list):\n",
    "            if partition == \"train\":\n",
    "                self.mfcc_dir = data_path + '/' + partition + \"/\" + mfcc_file + \"/\"\n",
    "                self.transcript_dir = data_path + '/' + partition + \"/labels/\"\n",
    "            else:\n",
    "                self.mfcc_dir = data_path + '/' + partition + \"/\" + mfcc_file + \"/\"\n",
    "                self.transcript_dir = data_path + '/' + partition + \"/labels/\"\n",
    "\n",
    "            mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
    "            transcript_names = sorted(os.listdir(self.transcript_dir))\n",
    "            assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "            self.mfccs = []\n",
    "\n",
    "            for i in range(0, len(mfcc_names)):\n",
    "    \n",
    "                mfcc = np.load(self.mfcc_dir + mfcc_names[i])\n",
    "                self.mfccs.append(mfcc)\n",
    "            #   Optionally do Cepstral Normalization of mfcc\n",
    "            #   Load the corresponding transcript\n",
    "                if index == 0:\n",
    "                    transcript = np.load(self.transcript_dir + transcript_names[i])\n",
    "                    self.transcripts.append(transcript)\n",
    "\n",
    "            # if partition == \"train\":\n",
    "            #     if shuffle == True:\n",
    "            #         Pairs = list(zip(self.mfccs, self.transcripts))\n",
    "            #         random.shuffle(Pairs)\n",
    "            #         self.mfccs, self.transcripts = zip(*Pairs)\n",
    "\n",
    "            # Each mfcc is of shape T1 x 20, T2 x 20, ...\n",
    "            # Each transcript is of shape (T1+2) x 20, (T2+2) x 20\n",
    "\n",
    "            # TODO: Concatenate all mfccs in self.X such that the final shape is T x 20 (Where T = T1 + T2 + ...) \n",
    "            self.mfccs = np.concatenate(self.mfccs, axis=0)\n",
    "            self.length = len(self.mfccs)\n",
    "            # TODO: Concatenate all transcripts in self.Y such that the final shape is (T,) meaning, each time step has one phoneme output\n",
    "            if index == 0:\n",
    "                self.transcripts = np.concatenate(self.transcripts, axis=0)\n",
    "            # Hint: Use numpy to concatenate\n",
    "            # Take some time to think about what we have done. self.mfcc is an array of the format (Frames x Features). Our goal is to recognize phonemes of each frame\n",
    "            # From hw0, you will be knowing what context is. We can introduce context by padding zeros on top and bottom of self.mfcc\n",
    "            if context != 0:\n",
    "                zero_paddings = np.zeros((context, 20))\n",
    "                up_paded = np.vstack((zero_paddings, self.mfccs))\n",
    "                down_paded = np.vstack((up_paded, zero_paddings))\n",
    "                self.mfccs = down_paded\n",
    "                \n",
    "            self.mfcc_list.append(self.mfccs)\n",
    "        self.mfccs = np.stack(self.mfcc_list)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \n",
    "        start_index = ind + self.offset - self.context\n",
    "        \n",
    "        ## Calculate ending timestep using offset and context (1 line)\n",
    "        end_index = ind + self.offset + self.context + 1\n",
    "\n",
    "        frames = self.mfccs[:, start_index:end_index, :]\n",
    "        \n",
    "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
    "        # After slicing, you get an array of shape 2*context+1 x 15. But our MLP needs 1d data and not 2d.\n",
    "        height = frames.shape[0]\n",
    "        width = frames.shape[1]\n",
    "        if self.normalization == True:\n",
    "          frames = frames - frames.mean(axis=0, keepdims=True)\n",
    "          # frames_variance = np.var(frames, axis=0)\n",
    "          # frames = np.divide(frames, np.tile(frames_variance, (height, 1)))\n",
    "        frames = torch.FloatTensor(frames) # Convert to tensors\n",
    "        onset = torch.tensor(self.transcripts[ind])       \n",
    "        \n",
    "        return frames, onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTestDataset(torch.utils.data.Dataset):\n",
    "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
    "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n",
    "    def __init__(self, data_path, context, offset=0, limit=-1, Normalization=True, mfcc_list=[\"mfcc\", \"mfcc_320\", \"mfcc_480\"]): # Feel free to add more arguments\n",
    "\n",
    "        self.context = context\n",
    "        self.offset = context\n",
    "        self.data_path = data_path\n",
    "        self.mfcc_dir = data_path + '/' + \"test\" + \"/mfcc/\"\n",
    "        self.normalization = Normalization\n",
    "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
    "        self.mfcc_list = mfcc_list\n",
    "        \n",
    "        self.mfcc_list = []\n",
    "        for mfcc_file in mfcc_list:\n",
    "            \n",
    "            self.mfccs= []\n",
    "            for i in range(0, len(mfcc_names)):\n",
    "            #   Load a single mfcc\n",
    "                mfcc = np.load(self.mfcc_dir + mfcc_names[i])\n",
    "                self.mfccs.append(mfcc)\n",
    "\n",
    "            # NOTE:\n",
    "            # Each mfcc is of shape T1 x 20, T2 x 20, ...\n",
    "            # Each transcript is of shape (T1+2) x 20, (T2+2) x 20 before removing [SOS] and [EOS]\n",
    "\n",
    "            self.mfccs = np.concatenate(self.mfccs, axis=0)\n",
    "            self.length = len(self.mfccs)\n",
    "\n",
    "            # Take some time to think about what we have done. self.mfcc is an array of the format (Frames x Features). Our goal is to recognize phonemes of each frame\n",
    "            # From hw0, you will be knowing what context is. We can introduce context by padding zeros on top and bottom of self.mfcc\n",
    "            if context != 0:\n",
    "                zero_paddings = np.zeros((context, 20))\n",
    "                up_paded = np.vstack((zero_paddings, self.mfccs))\n",
    "                down_paded = np.vstack((up_paded, zero_paddings))\n",
    "                self.mfccs = down_paded\n",
    "            \n",
    "            self.mfcc_list.append(self.mfccs)\n",
    "        self.mfccs = np.stack(self.mfcc_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "\n",
    "        start_index = ind + self.offset - self.context\n",
    "        ## Calculate ending timestep using offset and context (1 line)\n",
    "        end_index = ind + self.offset + self.context + 1\n",
    "        frames = self.mfccs[:, start_index:end_index, :]\n",
    "        height = frames.shape[0]\n",
    "        width = frames.shape[1]\n",
    "        if self.normalization == True:\n",
    "          frames = frames - frames.mean(axis=0, keepdims=True)\n",
    "\n",
    "        frames = torch.FloatTensor(frames) # Convert to tensors \n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs': 5,\n",
    "    'batch_size' : 512,\n",
    "    'context' : 15,\n",
    "    'learning_rate' : 0.001,\n",
    "    'architecture' : 'medium-cutoff'\n",
    "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = root\n",
    "train_data = AudioDataset(data_path, context = config['context'], offset=0, partition= \"train\", limit=-1)\n",
    "val_data = AudioDataset(data_path, context = config['context'], offset=0, partition= \"dev\", limit=-1, shuffle=False) \n",
    "test_data = AudioDataset(data_path, context = config['context'], offset=0, partition= \"test\", limit=-1, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  512\n",
      "Context:  15\n",
      "Input size:  620\n",
      "Train dataset samples = 126430, batches = 247\n",
      "Validation dataset samples = 17865, batches = 35\n",
      "Test dataset samples = 17977, batches = 36\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 4,\n",
    "                                           batch_size=config['batch_size'], pin_memory= True,\n",
    "                                           shuffle= True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
    "                                         batch_size=config['batch_size'], pin_memory= True,\n",
    "                                         shuffle= False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 2, \n",
    "                                          batch_size=config['batch_size'], pin_memory= True, \n",
    "                                          shuffle= False)\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Context: \", config['context'])\n",
    "print(\"Input size: \", (2*config['context']+1)*20)\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 31, 20]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(val_loader):\n",
    "    frames, onset = data\n",
    "    onset = torch.squeeze(onset, 1)\n",
    "    print(frames.shape, onset.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, context):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        input_size = 3 #Why is this the case?\n",
    "        num_classes = 2\n",
    "        dropout_rate = 0\n",
    "        self.conv1 = torch.nn.Conv2d(input_size, 10, kernel_size=(5, 3))\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=10)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(1, 3))\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=20)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=(1, 2))\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        \n",
    "        # self.backbone = torch.nn.Sequential(\n",
    "        #     torch.nn.Conv2d(input_size, 10, kernel_size=(5, 3)),\n",
    "        #     torch.nn.BatchNorm1d(num_features=10),\n",
    "        #     torch.nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        #     torch.nn.Conv2d(10, 20, kernel_size=3), \n",
    "        #     torch.nn.BatchNorm1d(num_features=20),\n",
    "        #     torch.nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        #     torch.nn.Flatten()\n",
    "        # )\n",
    "        \n",
    "        self.cls_layer = nn.Sequential(\n",
    "            torch.nn.Linear(in_features=1000, out_features=256),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(in_features=256, out_features=num_classes)\n",
    "        )          \n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.conv1(x)\n",
    "        feats = self.bn1(feats)\n",
    "        feats = self.pool1(feats)\n",
    "        feats = self.conv2(feats)\n",
    "        feats = self.bn2(feats)\n",
    "        feats = self.pool2(feats)\n",
    "        feats = self.flatten(feats)\n",
    "        \n",
    "        out = self.cls_layer(feats)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(config['context']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_amp = True\n",
    "criterion = torch.nn.CrossEntropyLoss() #Defining Loss function \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.05, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "# optimizer = AdamP(model.parameters(), lr=config['learning_rate']) #Defining Optimizer\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,20,25,30], gamma=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
    "    phone_true_list = []\n",
    "    phone_pred_list = []\n",
    "    phone_pred_score_list = []\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        frames, onsets = data\n",
    "        ### Move data to device (ideally GPU)\n",
    "        onsets = torch.squeeze(onsets, 1).long()\n",
    "        frames, onsets = frames.to(device), onsets.to(device) \n",
    "\n",
    "        with torch.inference_mode(): # makes sure that there are no gradients computed as we are not training the model now\n",
    "            ### Forward Propagation\n",
    "            logits = model(frames)\n",
    "            loss = criterion(logits, onsets)\n",
    "        \n",
    "        \n",
    "        val_loss = loss.item()\n",
    "\n",
    "        # val_acc = float(torch.sum(logits.argmax(axis=1) == onsets) / onsets.shape[0])\n",
    "        \n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # total_val_acc += float(torch.sum(logits.argmax(axis=1) == onsets) / onsets.shape[0])\n",
    "\n",
    "    #     batch_bar.set_postfix(\n",
    "    # acc=\"{:.04f}%\".format(val_acc),\n",
    "    # loss=\"{:.04f}\".format(val_loss))\n",
    "        ### Get Predictions\n",
    "        predicted_score = logits[:, 1]\n",
    "        phone_true_list.extend(onsets.cpu().tolist())\n",
    "        phone_pred_score_list.extend(predicted_score.cpu().tolist())\n",
    "        \n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "    \n",
    "        # total_val_loss /= len(dataloader)\n",
    "\n",
    "        # total_val_acc /= len(dataloader)\n",
    "        del frames, onsets, logits\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_bar.update()\n",
    "        \n",
    "    batch_bar.close()\n",
    "    total_val_loss /= len(dataloader)\n",
    "    \n",
    "    threshold = calculate_threshold(phone_true_list, phone_pred_score_list)\n",
    "    predicted_phonemes = phone_pred_score_list >= threshold\n",
    "\n",
    "    # total_val_acc /= len(dataloader)\n",
    "    ### Calculate Accuracy\n",
    "    accuracy = sklearn.metrics.accuracy_score(phone_true_list, predicted_phonemes) \n",
    "    auc = sklearn.metrics.roc_auc_score(phone_true_list, phone_pred_score_list)\n",
    "    auprc = sklearn.metrics.average_precision_score(phone_true_list, phone_pred_score_list)\n",
    "    f1 = sklearn.metrics.f1_score(phone_true_list, predicted_phonemes)\n",
    "    recall = sklearn.metrics.recall_score(phone_true_list, predicted_phonemes)\n",
    "    precision = sklearn.metrics.precision_score(phone_true_list, predicted_phonemes)\n",
    "    \n",
    "    return accuracy, total_val_loss, auc, auprc, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7a585c3524304a305745d85429513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model/model_checkpoint_pretrained_0.001_0.3_delta.pth\")[\"model_state_dict\"])\n",
    "eval_accuracy, eval_loss, eval_auroc, eval_auprc, eval_f1, eval_recall, eval_pre = eval(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6141180397174167  Precision: 0.21850019033117624    Recall: 0.6888   f1: 0.3317599460552933   AUROC: 0.684859972862958\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {eval_accuracy}  Precision: {eval_pre}    Recall: {eval_recall}   f1: {eval_f1}   AUROC: {eval_auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 16:21:59) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c0c70546662cbfb4501903633e87cb8f11618b84600ff2135b4bbfd529ed777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
